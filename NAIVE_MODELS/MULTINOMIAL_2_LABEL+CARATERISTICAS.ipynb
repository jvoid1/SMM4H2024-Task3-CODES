{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MULTINOMIAL 2 LABEL STEM COUNTVECTORIZER NLTK"
      ],
      "metadata": {
        "id": "NOuIlmLOrlqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import drive\n",
        "import joblib\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsGIVEhurlDY",
        "outputId": "00c2b829-63d1-49e2-a85e-fc9410db5757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar el stemmer en inglés\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Función para realizar stemming en el texto\n",
        "def stem_text(text):\n",
        "    # Tokenizar el texto\n",
        "    tokens = word_tokenize(text)\n",
        "    # Realizar stemming en cada token y excluir las stopwords\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "    return ' '.join(stemmed_words)\n"
      ],
      "metadata": {
        "id": "o9nGjwJHr0Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos\n",
        "data = pd.read_csv('SMM4H_2024_Task3_Training_1800.csv', nrows=1800, usecols=[1, 2, 3], engine='python')\n"
      ],
      "metadata": {
        "id": "BfZV1-d0r4aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lematizar las columnas 'keyword' y 'text'\n",
        "data['keyword'] = data['keyword'].apply(stem_text)\n",
        "data['text'] = data['text'].apply(stem_text)\n",
        "\n",
        "# Reemplazar los valores de la columna 'label' que no son 0 por 1\n",
        "data['label'] = data['label'].apply(lambda x: 1 if x != 0 else x)\n"
      ],
      "metadata": {
        "id": "aEe3ftVRr7fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Partición de los datos en train, validation y test\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)"
      ],
      "metadata": {
        "id": "OyvajkxGr-o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesamiento de texto y construcción del modelo\n",
        "vectorizer = CountVectorizer()\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Entrenar el modelo\n",
        "X_train = vectorizer.fit_transform(train_data['text'])\n",
        "y_train = train_data['label']\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Validación del modelo\n",
        "X_val = vectorizer.transform(val_data['text'])\n",
        "y_val = val_data['label']\n",
        "predicted_labels = model.predict(X_val)\n"
      ],
      "metadata": {
        "id": "y384iMT5sBOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo\n",
        "print(\"Resultados en datos de validación:\")\n",
        "print(classification_report(y_val, predicted_labels))\n",
        "\n",
        "# Prueba del modelo\n",
        "X_test = vectorizer.transform(test_data['text'])\n",
        "y_test = test_data['label']\n",
        "predicted_labels_test = model.predict(X_test)\n",
        "\n",
        "# Evaluación del modelo en los datos de prueba\n",
        "print(\"\\nResultados en datos de prueba:\")\n",
        "print(classification_report(y_test, predicted_labels_test))\n",
        "\n",
        "# Guardar el modelo en un archivo\n",
        "joblib.dump(model, 'naive_bayes_model_multinomial_stem_count_nltk.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer_multinomial_stem_count_nltk.pkl')\n",
        "\n",
        "# Copiar el archivo del modelo a Google Drive\n",
        "!cp naive_bayes_model_multinomial_stem_count_nltk.pkl '/content/drive/My Drive/'\n",
        "!cp vectorizer_multinomial_stem_count_nltk.pkl '/content/drive/My Drive/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L4Rz_yRsHLP",
        "outputId": "2a3af230-424e-4d29-a99c-ecd4fa8d535d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados en datos de validación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.81      0.80       113\n",
            "           1       0.67      0.63      0.65        67\n",
            "\n",
            "    accuracy                           0.74       180\n",
            "   macro avg       0.73      0.72      0.72       180\n",
            "weighted avg       0.74      0.74      0.74       180\n",
            "\n",
            "\n",
            "Resultados en datos de prueba:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.74      0.74       113\n",
            "           1       0.56      0.55      0.56        67\n",
            "\n",
            "    accuracy                           0.67       180\n",
            "   macro avg       0.65      0.65      0.65       180\n",
            "weighted avg       0.67      0.67      0.67       180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "CON PROMEDIO DE PALABRAS KEYWORD/LABEL TFIDFVECTORIZER"
      ],
      "metadata": {
        "id": "73zl6rK3vKfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Descargar el recurso 'stopwords'\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Inicializar el stemmer en inglés\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Función para realizar stemming en el texto\n",
        "def stem_text(text):\n",
        "    # Tokenizar el texto\n",
        "    tokens = word_tokenize(text)\n",
        "    # Realizar stemming en cada token y excluir las stopwords\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Cargar los datos\n",
        "data = pd.read_csv('SMM4H_2024_Task3_Training_1800.csv', nrows=1800, usecols=[1, 2, 3], engine='python')\n",
        "\n",
        "# Lematizar las columnas 'keyword' y 'text'\n",
        "data['keyword'] = data['keyword'].apply(stem_text)\n",
        "data['text'] = data['text'].apply(stem_text)\n",
        "\n",
        "# Reemplazar los valores de la columna 'label' que no son 0 por 1\n",
        "data['label'] = data['label'].apply(lambda x: 1 if x != 0 else x)\n",
        "\n",
        "# Calcular el promedio de palabras en el texto para cada clase\n",
        "data['word_count'] = data['text'].apply(lambda x: len(x.split()))\n",
        "class_word_counts = data.groupby('label')['word_count'].mean()\n",
        "\n",
        "# Normalizar los valores del promedio de palabras\n",
        "max_word_count = class_word_counts.max()\n",
        "min_word_count = class_word_counts.min()\n",
        "normalized_class_word_counts = (class_word_counts - min_word_count) / (max_word_count - min_word_count)\n",
        "\n",
        "# Añadir la característica normalizada como una columna adicional en los datos\n",
        "train_data['normalized_word_count'] = train_data['label'].map(normalized_class_word_counts)\n",
        "val_data['normalized_word_count'] = val_data['label'].map(normalized_class_word_counts)\n",
        "test_data['normalized_word_count'] = test_data['label'].map(normalized_class_word_counts)\n",
        "\n",
        "# Partición de los datos en train, validation y test\n",
        "X_train = train_data[['keyword', 'normalized_word_count']]\n",
        "X_val = val_data[['keyword', 'normalized_word_count']]\n",
        "X_test = test_data[['keyword', 'normalized_word_count']]\n",
        "y_train = train_data['label']\n",
        "y_val = val_data['label']\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Preprocesamiento de texto y construcción del modelo\n",
        "vectorizer = TfidfVectorizer()\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Entrenar el modelo\n",
        "X_train_text = vectorizer.fit_transform(X_train['keyword'])\n",
        "X_train_combined = np.hstack((X_train_text.toarray(), np.array(X_train['normalized_word_count']).reshape(-1, 1)))\n",
        "model.fit(X_train_combined, y_train)\n",
        "\n",
        "# Validación del modelo\n",
        "X_val_text = vectorizer.transform(X_val['keyword'])\n",
        "X_val_combined = np.hstack((X_val_text.toarray(), np.array(X_val['normalized_word_count']).reshape(-1, 1)))\n",
        "predicted_labels = model.predict(X_val_combined)\n",
        "\n",
        "# Evaluación del modelo\n",
        "print(\"Resultados en datos de validación:\")\n",
        "print(classification_report(y_val, predicted_labels))\n",
        "\n",
        "# Prueba del modelo\n",
        "X_test_text = vectorizer.transform(X_test['keyword'])\n",
        "X_test_combined = np.hstack((X_test_text.toarray(), np.array(X_test['normalized_word_count']).reshape(-1, 1)))\n",
        "predicted_labels_test = model.predict(X_test_combined)\n",
        "\n",
        "# Evaluación del modelo en los datos de prueba\n",
        "print(\"\\nResultados en datos de prueba:\")\n",
        "print(classification_report(y_test, predicted_labels_test))\n",
        "\n",
        "# Guardar el modelo en un archivo\n",
        "joblib.dump(model, 'naive_bayes_model_multinomial_prom_pal_keyword_label_tfidf.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer_multinomial_prom_pal_keyword_label_tfidf.pkl')\n",
        "\n",
        "# Copiar el archivo del modelo a Google Drive\n",
        "!cp naive_bayes_model_multinomial_prom_pal_keyword_label_tfidf.pkl '/content/drive/My Drive/'\n",
        "!cp vectorizer_multinomial_prom_pal_keyword_label_tfidf.pkl '/content/drive/My Drive/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5tjBeiyvJx1",
        "outputId": "3c6b2a14-b185-4e8a-f75c-4d12685a6747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados en datos de validación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.99       113\n",
            "           1       0.96      1.00      0.98        67\n",
            "\n",
            "    accuracy                           0.98       180\n",
            "   macro avg       0.98      0.99      0.98       180\n",
            "weighted avg       0.98      0.98      0.98       180\n",
            "\n",
            "\n",
            "Resultados en datos de prueba:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       113\n",
            "           1       0.97      1.00      0.99        67\n",
            "\n",
            "    accuracy                           0.99       180\n",
            "   macro avg       0.99      0.99      0.99       180\n",
            "weighted avg       0.99      0.99      0.99       180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "CON PROMEDIO DE PALABRAS KEYWORD/LABEL COUNTVECTORIZER\n",
        "\n"
      ],
      "metadata": {
        "id": "Qsapnf_7v8An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Descargar el recurso 'stopwords'\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Inicializar el stemmer en inglés\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Función para realizar stemming en el texto\n",
        "def stem_text(text):\n",
        "    # Tokenizar el texto\n",
        "    tokens = word_tokenize(text)\n",
        "    # Realizar stemming en cada token y excluir las stopwords\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Cargar los datos\n",
        "data = pd.read_csv('SMM4H_2024_Task3_Training_1800.csv', nrows=1800, usecols=[1, 2, 3], engine='python')\n",
        "\n",
        "# Lematizar las columnas 'keyword' y 'text'\n",
        "data['keyword'] = data['keyword'].apply(stem_text)\n",
        "data['text'] = data['text'].apply(stem_text)\n",
        "\n",
        "# Reemplazar los valores de la columna 'label' que no son 0 por 1\n",
        "data['label'] = data['label'].apply(lambda x: 1 if x != 0 else x)\n",
        "\n",
        "# Calcular el promedio de palabras en el texto para cada clase\n",
        "data['word_count'] = data['text'].apply(lambda x: len(x.split()))\n",
        "class_word_counts = data.groupby('label')['word_count'].mean()\n",
        "\n",
        "# Normalizar los valores del promedio de palabras\n",
        "max_word_count = class_word_counts.max()\n",
        "min_word_count = class_word_counts.min()\n",
        "normalized_class_word_counts = (class_word_counts - min_word_count) / (max_word_count - min_word_count)\n",
        "\n",
        "# Añadir la característica normalizada como una columna adicional en los datos\n",
        "train_data['normalized_word_count'] = train_data['label'].map(normalized_class_word_counts)\n",
        "val_data['normalized_word_count'] = val_data['label'].map(normalized_class_word_counts)\n",
        "test_data['normalized_word_count'] = test_data['label'].map(normalized_class_word_counts)\n",
        "\n",
        "# Partición de los datos en train, validation y test\n",
        "X_train = train_data[['keyword', 'normalized_word_count']]\n",
        "X_val = val_data[['keyword', 'normalized_word_count']]\n",
        "X_test = test_data[['keyword', 'normalized_word_count']]\n",
        "y_train = train_data['label']\n",
        "y_val = val_data['label']\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Preprocesamiento de texto y construcción del modelo\n",
        "vectorizer = CountVectorizer()\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Entrenar el modelo\n",
        "X_train_text = vectorizer.fit_transform(X_train['keyword'])\n",
        "X_train_combined = np.hstack((X_train_text.toarray(), np.array(X_train['normalized_word_count']).reshape(-1, 1)))\n",
        "model.fit(X_train_combined, y_train)\n",
        "\n",
        "# Validación del modelo\n",
        "X_val_text = vectorizer.transform(X_val['keyword'])\n",
        "X_val_combined = np.hstack((X_val_text.toarray(), np.array(X_val['normalized_word_count']).reshape(-1, 1)))\n",
        "predicted_labels = model.predict(X_val_combined)\n",
        "\n",
        "# Evaluación del modelo\n",
        "print(\"Resultados en datos de validación:\")\n",
        "print(classification_report(y_val, predicted_labels))\n",
        "\n",
        "# Prueba del modelo\n",
        "X_test_text = vectorizer.transform(X_test['keyword'])\n",
        "X_test_combined = np.hstack((X_test_text.toarray(), np.array(X_test['normalized_word_count']).reshape(-1, 1)))\n",
        "predicted_labels_test = model.predict(X_test_combined)\n",
        "\n",
        "# Evaluación del modelo en los datos de prueba\n",
        "print(\"\\nResultados en datos de prueba:\")\n",
        "print(classification_report(y_test, predicted_labels_test))\n",
        "\n",
        "# Guardar el modelo en un archivo\n",
        "joblib.dump(model, 'naive_bayes_model_multinomial_prom_pal_keyword_label_count.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer_multinomial_prom_pal_keyword_label_count.pkl')\n",
        "\n",
        "# Copiar el archivo del modelo a Google Drive\n",
        "!cp naive_bayes_model_multinomial_prom_pal_keyword_label_count.pkl '/content/drive/My Drive/'\n",
        "!cp vectorizer_multinomial_prom_pal_keyword_label_count.pkl '/content/drive/My Drive/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHgenpzAv7aI",
        "outputId": "7d0b3025-7340-4570-ec2e-bd84ae94fed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados en datos de validación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98       113\n",
            "           1       0.93      1.00      0.96        67\n",
            "\n",
            "    accuracy                           0.97       180\n",
            "   macro avg       0.97      0.98      0.97       180\n",
            "weighted avg       0.97      0.97      0.97       180\n",
            "\n",
            "\n",
            "Resultados en datos de prueba:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.99       113\n",
            "           1       0.96      1.00      0.98        67\n",
            "\n",
            "    accuracy                           0.98       180\n",
            "   macro avg       0.98      0.99      0.98       180\n",
            "weighted avg       0.98      0.98      0.98       180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "CON PROMEDIO DE PALABRAS TEXT/LABEL COUNTVECTORIZER\n",
        "\n"
      ],
      "metadata": {
        "id": "2vQAWoY8wXt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Descargar el recurso 'stopwords'\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Inicializar el stemmer en inglés\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Función para realizar stemming en el texto\n",
        "def stem_text(text):\n",
        "    # Tokenizar el texto\n",
        "    tokens = word_tokenize(text)\n",
        "    # Realizar stemming en cada token y excluir las stopwords\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Cargar los datos\n",
        "data = pd.read_csv('SMM4H_2024_Task3_Training_1800.csv', nrows=1800, usecols=[1, 2, 3], engine='python')\n",
        "\n",
        "# Lematizar las columnas 'keyword' y 'text'\n",
        "data['keyword'] = data['keyword'].apply(stem_text)\n",
        "data['text'] = data['text'].apply(stem_text)\n",
        "\n",
        "# Reemplazar los valores de la columna 'label' que no son 0 por 1\n",
        "data['label'] = data['label'].apply(lambda x: 1 if x != 0 else x)\n",
        "\n",
        "# Calcular el promedio de palabras en el texto para cada clase\n",
        "data['word_count'] = data['text'].apply(lambda x: len(x.split()))\n",
        "class_word_counts = data.groupby('label')['word_count'].mean()\n",
        "\n",
        "# Normalizar los valores del promedio de palabras\n",
        "max_word_count = class_word_counts.max()\n",
        "min_word_count = class_word_counts.min()\n",
        "normalized_class_word_counts = (class_word_counts - min_word_count) / (max_word_count - min_word_count)\n",
        "\n",
        "# Añadir la característica normalizada como una columna adicional en los datos\n",
        "train_data['normalized_word_count'] = train_data['label'].map(normalized_class_word_counts)\n",
        "val_data['normalized_word_count'] = val_data['label'].map(normalized_class_word_counts)\n",
        "test_data['normalized_word_count'] = test_data['label'].map(normalized_class_word_counts)\n",
        "\n",
        "# Partición de los datos en train, validation y test\n",
        "X_train = train_data[['text', 'normalized_word_count']]\n",
        "X_val = val_data[['text', 'normalized_word_count']]\n",
        "X_test = test_data[['text', 'normalized_word_count']]\n",
        "y_train = train_data['label']\n",
        "y_val = val_data['label']\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Preprocesamiento de texto y construcción del modelo\n",
        "vectorizer = CountVectorizer()\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Entrenar el modelo\n",
        "X_train_text = vectorizer.fit_transform(X_train['text'])\n",
        "X_train_combined = np.hstack((X_train_text.toarray(), np.array(X_train['normalized_word_count']).reshape(-1, 1)))\n",
        "model.fit(X_train_combined, y_train)\n",
        "\n",
        "# Validación del modelo\n",
        "X_val_text = vectorizer.transform(X_val['text'])\n",
        "X_val_combined = np.hstack((X_val_text.toarray(), np.array(X_val['normalized_word_count']).reshape(-1, 1)))\n",
        "predicted_labels = model.predict(X_val_combined)\n",
        "\n",
        "# Evaluación del modelo\n",
        "print(\"Resultados en datos de validación:\")\n",
        "print(classification_report(y_val, predicted_labels))\n",
        "\n",
        "# Prueba del modelo\n",
        "X_test_text = vectorizer.transform(X_test['text'])\n",
        "X_test_combined = np.hstack((X_test_text.toarray(), np.array(X_test['normalized_word_count']).reshape(-1, 1)))\n",
        "predicted_labels_test = model.predict(X_test_combined)\n",
        "\n",
        "# Evaluación del modelo en los datos de prueba\n",
        "print(\"\\nResultados en datos de prueba:\")\n",
        "print(classification_report(y_test, predicted_labels_test))\n",
        "\n",
        "# Guardar el modelo en un archivo\n",
        "joblib.dump(model, 'naive_bayes_model_multinomial_prom_pal_text_label_count.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer_multinomial_prom_pal_text_label_count.pkl')\n",
        "\n",
        "# Copiar el archivo del modelo a Google Drive\n",
        "!cp naive_bayes_model_multinomial_prom_pal_text_label_count.pkl '/content/drive/My Drive/'\n",
        "!cp vectorizer_multinomial_prom_pal_text_label_count.pkl '/content/drive/My Drive/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRdeqU1GwTlh",
        "outputId": "ee472b05-5405-46c0-9021-748b703a6ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados en datos de validación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.86      0.90       113\n",
            "           1       0.79      0.91      0.85        67\n",
            "\n",
            "    accuracy                           0.88       180\n",
            "   macro avg       0.87      0.88      0.87       180\n",
            "weighted avg       0.89      0.88      0.88       180\n",
            "\n",
            "\n",
            "Resultados en datos de prueba:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.80      0.85       113\n",
            "           1       0.71      0.85      0.78        67\n",
            "\n",
            "    accuracy                           0.82       180\n",
            "   macro avg       0.81      0.82      0.81       180\n",
            "weighted avg       0.83      0.82      0.82       180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "CON PROMEDIO DE PALABRAS TEXT/LABEL TFIDFVECTORIZER\n"
      ],
      "metadata": {
        "id": "hM6s5uLMwnrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Descargar el recurso 'stopwords'\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Inicializar el stemmer en inglés\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Función para realizar stemming en el texto\n",
        "def stem_text(text):\n",
        "    # Tokenizar el texto\n",
        "    tokens = word_tokenize(text)\n",
        "    # Realizar stemming en cada token y excluir las stopwords\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Cargar los datos\n",
        "data = pd.read_csv('SMM4H_2024_Task3_Training_1800.csv', nrows=1800, usecols=[1, 2, 3], engine='python')\n",
        "\n",
        "# Lematizar las columnas 'keyword' y 'text'\n",
        "data['keyword'] = data['keyword'].apply(stem_text)\n",
        "data['text'] = data['text'].apply(stem_text)\n",
        "\n",
        "# Reemplazar los valores de la columna 'label' que no son 0 por 1\n",
        "data['label'] = data['label'].apply(lambda x: 1 if x != 0 else x)\n",
        "\n",
        "# Calcular el promedio de palabras en el texto para cada clase\n",
        "data['word_count'] = data['text'].apply(lambda x: len(x.split()))\n",
        "class_word_counts = data.groupby('label')['word_count'].mean()\n",
        "print(class_word_counts)\n",
        "\n",
        "# Normalizar los valores del promedio de palabras\n",
        "max_word_count = class_word_counts.max()\n",
        "min_word_count = class_word_counts.min()\n",
        "normalized_class_word_counts = (class_word_counts - min_word_count) / (max_word_count - min_word_count)\n",
        "\n",
        "# Añadir la característica normalizada como una columna adicional en los datos\n",
        "train_data['normalized_word_count'] = train_data['label'].map(normalized_class_word_counts)\n",
        "val_data['normalized_word_count'] = val_data['label'].map(normalized_class_word_counts)\n",
        "test_data['normalized_word_count'] = test_data['label'].map(normalized_class_word_counts)\n",
        "\n",
        "# Partición de los datos en train, validation y test\n",
        "X_train = train_data[['text', 'normalized_word_count']]\n",
        "X_val = val_data[['text', 'normalized_word_count']]\n",
        "X_test = test_data[['text', 'normalized_word_count']]\n",
        "y_train = train_data['label']\n",
        "y_val = val_data['label']\n",
        "y_test = test_data['label']\n",
        "\n",
        "print('test_data',X_test)\n",
        "\n",
        "# Preprocesamiento de texto y construcción del modelo\n",
        "vectorizer = TfidfVectorizer()\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Entrenar el modelo\n",
        "X_train_text = vectorizer.fit_transform(X_train['text'])\n",
        "X_train_combined = np.hstack((X_train_text.toarray(), np.array(X_train['normalized_word_count']).reshape(-1, 1)))\n",
        "model.fit(X_train_combined, y_train)\n",
        "\n",
        "# Validación del modelo\n",
        "X_val_text = vectorizer.transform(X_val['text'])\n",
        "X_val_combined = np.hstack((X_val_text.toarray(), np.array(X_val['normalized_word_count']).reshape(-1, 1)))\n",
        "predicted_labels = model.predict(X_val_combined)\n",
        "\n",
        "# Evaluación del modelo\n",
        "print(\"Resultados en datos de validación:\")\n",
        "print(classification_report(y_val, predicted_labels))\n",
        "\n",
        "# Prueba del modelo\n",
        "X_test_text = vectorizer.transform(X_test['text'])\n",
        "X_test_combined = np.hstack((X_test_text.toarray(), np.array(X_test['normalized_word_count']).reshape(-1, 1)))\n",
        "predicted_labels_test = model.predict(X_test_combined)\n",
        "\n",
        "# Evaluación del modelo en los datos de prueba\n",
        "print(\"\\nResultados en datos de prueba:\")\n",
        "print(classification_report(y_test, predicted_labels_test))\n",
        "\n",
        "# Guardar el modelo en un archivo\n",
        "joblib.dump(model, 'naive_bayes_model_multinomial_prom_pal_text_label_tfidf.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer_multinomial_prom_pal_text_label_tfidf.pkl')\n",
        "\n",
        "# Copiar el archivo del modelo a Google Drive\n",
        "!cp naive_bayes_model_multinomial_prom_pal_text_label_tfidf.pkl '/content/drive/My Drive/'\n",
        "!cp vectorizer_multinomial_prom_pal_text_label_tfidf.pkl '/content/drive/My Drive/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNJoxs4AwnI8",
        "outputId": "49764c48-cb8f-4962-c8bf-beb22aad818a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    111.706454\n",
            "1    150.692078\n",
            "Name: word_count, dtype: float64\n",
            "test_data                                                    text  normalized_word_count\n",
            "1573  feel ya buddi , spent day loung around home , ...                    1.0\n",
            "460   sure tell regard possibl swim problem futur , ...                    0.0\n",
            "315   although subject frustrat , n't let beauti thi...                    0.0\n",
            "1659  struggl get outsid . start thing : 'm 21 year ...                    1.0\n",
            "1179  recent went run / walk . took like week actual...                    1.0\n",
            "...                                                 ...                    ...\n",
            "161   sever anxiety/depress . 'm 20 year old dude di...                    0.0\n",
            "654   anxieti roof ! . past week two get rid one dog...                    0.0\n",
            "957   n't realli tast music . younger probabl tast w...                    0.0\n",
            "744                         ’ outsid though . ’ peopl .                    0.0\n",
            "619   pretti much , 's exact get thing done make com...                    0.0\n",
            "\n",
            "[180 rows x 2 columns]\n",
            "Resultados en datos de validación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       113\n",
            "           1       1.00      1.00      1.00        67\n",
            "\n",
            "    accuracy                           1.00       180\n",
            "   macro avg       1.00      1.00      1.00       180\n",
            "weighted avg       1.00      1.00      1.00       180\n",
            "\n",
            "\n",
            "Resultados en datos de prueba:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       113\n",
            "           1       1.00      1.00      1.00        67\n",
            "\n",
            "    accuracy                           1.00       180\n",
            "   macro avg       1.00      1.00      1.00       180\n",
            "weighted avg       1.00      1.00      1.00       180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformar los datos de texto de todos los datos utilizando el mismo vectorizador\n",
        "X_all_data_text = vectorizer.transform(data['text'])\n",
        "\n",
        "# Calcular el promedio de palabras en el texto para cada clase\n",
        "data['word_count'] = data['text'].apply(lambda x: len(x.split()))\n",
        "class_word_counts = data.groupby('label')['word_count'].mean()\n",
        "\n",
        "# Normalizar los valores del promedio de palabras\n",
        "max_word_count = class_word_counts.max()\n",
        "min_word_count = class_word_counts.min()\n",
        "normalized_class_word_counts = (class_word_counts - min_word_count) / (max_word_count - min_word_count)\n",
        "\n",
        "# Añadir la característica normalizada como una columna adicional en los datos\n",
        "data['normalized_word_count'] = data['label'].map(normalized_class_word_counts)\n",
        "\n",
        "# Transformar los datos de texto de todos los datos utilizando el mismo vectorizador\n",
        "X_all_data_text = vectorizer.transform(data['text'])\n",
        "\n",
        "# Combinar las características de texto y la característica normalizada\n",
        "X_all_data_combined = np.hstack((X_all_data_text.toarray(), np.array(data['normalized_word_count']).reshape(-1, 1)))\n",
        "\n",
        "# Predecir las etiquetas para todos los datos\n",
        "predicted_labels_all_data = model.predict(X_all_data_combined)\n",
        "\n",
        "# Evaluación del modelo en todos los datos\n",
        "print(\"\\nResultados en todos los datos:\")\n",
        "print(classification_report(data['label'], predicted_labels_all_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvUIADrQzvB3",
        "outputId": "ba6868b9-bf11-4cd2-b3e1-3dac30c59f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resultados en todos los datos:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1131\n",
            "           1       1.00      1.00      1.00       669\n",
            "\n",
            "    accuracy                           1.00      1800\n",
            "   macro avg       1.00      1.00      1.00      1800\n",
            "weighted avg       1.00      1.00      1.00      1800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el archivo de prueba sin las etiquetas de clase\n",
        "test_data = pd.read_csv('SMM4H_2024_Task3_Validation_600_release.csv', nrows=600, usecols=[0, 1, 2], engine='python')\n",
        "\n",
        "# Lematizar las columnas 'keyword' y 'text'\n",
        "test_data['keyword'] = test_data['keyword'].apply(stem_text)\n",
        "test_data['text'] = test_data['text'].apply(stem_text)\n",
        "\n",
        "# Calcular el promedio de palabras en el texto para cada clase\n",
        "test_data['word_count'] = test_data['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "print(test_data)\n",
        "\n",
        "# Normalizar los valores del promedio de palabras\n",
        "test_data['normalized_word_count'] = (test_data['word_count'] - min_word_count) / (max_word_count - min_word_count)\n",
        "\n",
        "# Transformar los datos de texto utilizando el mismo vectorizador que se ajustó en los datos de entrenamiento\n",
        "X_test_text = vectorizer.transform(test_data['text'])\n",
        "\n",
        "# Combinar las características de texto con la característica normalizada\n",
        "X_test_combined = np.hstack((X_test_text.toarray(), np.array(test_data['normalized_word_count']).reshape(-1, 1)))\n",
        "\n",
        "# Predecir las etiquetas para los datos de prueba\n",
        "predicted_labels_test = model.predict(X_test_combined)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"\\nResultados en los nuevos datos:\")\n",
        "for text, predicted_label in zip(test_data['text'], predicted_labels_test):\n",
        "    print(f\"Texto: {text}\\nClase predicha: {predicted_label}\\n\")\n",
        "\n",
        "# Contar la frecuencia de cada clase predicha\n",
        "class_counts = np.bincount(predicted_labels_test)\n",
        "\n",
        "# Mostrar el conteo de cada clase\n",
        "for label, count in enumerate(class_counts):\n",
        "    print(f\"Clase {label}: {count} instancias\")\n"
      ],
      "metadata": {
        "id": "hGNLRPMn4yro",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9eWiQx0_9ZYh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}